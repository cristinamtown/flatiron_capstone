{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, data for the recommendation system will be scraped from letterboxd.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letterboxd Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Username list\n",
    "\n",
    "This function generates a list of the users that follow the chosen user. It scrapes up to x number of pages of usernames from Letterboxd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def users_list(username, pages_to_scrape):\n",
    "    '''Returns a list of users that follow the given username, input the number\n",
    "    of following pages you wish to scrape as an int.'''\n",
    "    start = time.time()\n",
    "\n",
    "    # First page of usernames\n",
    "    html_page = requests.get('https://letterboxd.com/'+ username + '/followers/page/1') \n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "    # Create list of usernames\n",
    "    users = [movie.find('a', class_='avatar -a40')['href'] \n",
    "             for movie in soup.find_all('div', class_='person-summary')]\n",
    "\n",
    "    # Add pages 2-pages_to_scrape to list of usernames\n",
    "    for x in range(2, pages_to_scrape):\n",
    "        x = str(x)\n",
    "        html_page = requests.get('https://letterboxd.com/'+ username + '/followers/page/' + x ) \n",
    "        soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    \n",
    "        users = users + [movie.find('a', class_='avatar -a40')['href']\n",
    "                          for movie in soup.find_all('div', class_='person-summary')]\n",
    "    \n",
    "    \n",
    "    # Add original username to list\n",
    "    users.append(username)\n",
    "\n",
    "    # Strip the '/' from the usernames\n",
    "    users = [elem.strip('/') for elem in users]\n",
    "    print(f'number of usernames: {len(users)}')\n",
    "\n",
    "    # Print run time\n",
    "    end = time.time()\n",
    "    print(f'time to run: {round((end-start), 2)}')\n",
    "    return users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My chosen user is 'fuchsiadyke' due to the fact that they are followed by over 16,000 users ensuring that we will get the 100 full pages of usernames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the above function to get the list of usernames\n",
    "users = users_list('fuchsiadyke', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick look at the first 10 values in the list\n",
    "users[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Rating DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes in a usernames gathers the all the rating information from the movies they have marked as seen on Letterboxd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_rating_3_col(username):\n",
    "    '''Scrapes user ratings from Letterboxd.com and creates a dataframe with\n",
    "    three columns: film_id, username, and rating'''\n",
    "\n",
    "    # Scrape the first page of films watched by username\n",
    "    html_page = requests.get('https://letterboxd.com/'+ username + '/films/page/' + '1' )\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    # Find the last page number of films watched by usename\n",
    "\n",
    "    try: \n",
    "        pages = soup.find('div', class_=\"paginate-pages\")\n",
    "        last_page = pages.find_all('li', class_='paginate-page')\n",
    "        last_page = last_page[-1].text\n",
    "    except:\n",
    "        last_page = 1\n",
    "    \n",
    "\n",
    "    # Create dictionary of film_id, film_name, and link\n",
    "    movies = [{'film_id': movie.find('div', class_='film-poster')['data-film-id']} \n",
    "              for movie in soup.find_all('li', class_='poster-container')]\n",
    "\n",
    "    # Create Dataframe with film_id, film_name, link\n",
    "    df_temp1 = pd.DataFrame(movies)\n",
    "\n",
    "    \n",
    "    film_rating = []\n",
    "    user_rating = []\n",
    "\n",
    "    # Create list of star rating\n",
    "    for x in soup.find_all('li', class_='poster-container'):\n",
    "        try: \n",
    "            film_rating.append(x.find('span', class_='rating').text)\n",
    "        except: \n",
    "            film_rating.append(None)\n",
    "\n",
    "    # Change star rating to number rating        \n",
    "    for x in film_rating:\n",
    "        try:\n",
    "            if x[-1]=='½':\n",
    "                user_rating.append(len(x)-0.5)\n",
    "            else:\n",
    "                user_rating.append(len(x))\n",
    "        except:\n",
    "            user_rating.append(None)\n",
    "            \n",
    "    # Add user_rating to the df\n",
    "    df_temp1['rating'] = user_rating\n",
    "    df_temp1['username'] = username\n",
    "    \n",
    "    # Scrape remaining pages and add to Dataframe\n",
    "    lp = int(last_page)\n",
    "\n",
    "    for y in range(2, lp+1):\n",
    "        z = str(y)\n",
    "    \n",
    "        html_page2 = requests.get('https://letterboxd.com/'+ username + '/films/page/' + z )\n",
    "        soup2 = BeautifulSoup(html_page2.content, 'html.parser')\n",
    "        \n",
    "        # Create dictionary of film_id, film_name, and link\n",
    "        movies2 = [{'film_id': movie.find('div', class_='film-poster')['data-film-id']} \n",
    "                  for movie in soup2.find_all('li', class_='poster-container')]\n",
    "        df_temp = pd.DataFrame(movies2)\n",
    "    \n",
    "        film_rating2 = []\n",
    "        user_rating2 = []\n",
    "        \n",
    "        # Create list of star rating\n",
    "        for x in soup2.find_all('li', class_='poster-container'):\n",
    "            try: \n",
    "                film_rating2.append(x.find('span', class_='rating').text)\n",
    "            except: \n",
    "                film_rating2.append(None)\n",
    "\n",
    "        # Change star rating to number rating        \n",
    "        for x in film_rating2:\n",
    "            try:\n",
    "                if x[-1]=='½':\n",
    "                    user_rating2.append(len(x)-0.5)\n",
    "                else:\n",
    "                    user_rating2.append(len(x))\n",
    "            except:\n",
    "                user_rating2.append(None)\n",
    "            \n",
    "        # add user_rating to the df\n",
    "        df_temp['rating'] = user_rating2\n",
    "        df_temp['username'] = username\n",
    "        \n",
    "        \n",
    "        df_temp1 = df_temp1.append(df_temp)\n",
    "        \n",
    "    return df_temp1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of test_users to ensure the function works before running on \n",
    "# all usernames\n",
    "\n",
    "test_users = ['ingloriousbasta', 'eldodo', 'tldr_com', 'yangforyin', 'iutub',\n",
    "              'willmsfilms', 'rockz', 'cae_des', 'ca2ba2', 'nischal170', 'javidog', \n",
    "              'irokill', 'ithacuss', 'ngcaihui42', 'mrireilly', 'danyalahmed',\n",
    "              'bortex', 'travishenderson', 'manuelcouto', 'sunsetsofie', 'nataliedc',\n",
    "              'badsioop', 'thewatchmakers', 'tashk', 'redgravehepburn', 'sadfrog23',\n",
    "              'sargy7', 'kna1223', 'janvite', 'privateidahos', 'mirels', 'dovegirl',\n",
    "              'adamcbest', 'sergioaudelo', 'muzwot', 'ethanjame', 'tylerharris',\n",
    "              'bugix', 'stephensboyer', 'kamikazegirls', 'alexisthegay', 'waster', \n",
    "              'seymacetin', 'arent', 'panizzz', 'phillitj', 'midnightnostalg', \n",
    "              'rkrespin', 'spikeydlux', 'sbernstein9', 'cryptidpeep', 'lenilinden',\n",
    "              'jomes', 'groenbaek', 'elisabethmcl', 'jasonmcghan', 'lostasterisk', \n",
    "              'fridge_lp', 'chiarahp', 'jmitchell67', 'dpen42', 'dubsdeedubs', \n",
    "              'piaescobar', 'jgaffney', 'jayhayes05', 'beatrixralph1', \n",
    "              'mickeemouser', '12monkeys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A for loop that runs through the list of usernames and the previous function\n",
    "# to create a dataframe of their ratings\n",
    "start = time.time()\n",
    "\n",
    "df = pd.DataFrame(columns=['film_id'])\n",
    "\n",
    "for user in users:\n",
    "    try:\n",
    "        temp = user_rating_3_col(user)\n",
    "        df = df.append(temp)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "end = time.time()\n",
    "    \n",
    "print(f'This took {round((end - start), 2)} seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look to make sure it work as intended\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns into how we will use them in the models.\n",
    "df = df[['username', 'film_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as a .csv file\n",
    "df.to_csv(r'panda_dataframes/user_rating_3col.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame for Film Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function scrapes the given number of pages from the letterboxd.com's most popular films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_names(last_page):\n",
    "    \"\"\"Scrapes film names from letterboxd.com. It takes the last_page to\n",
    "    be scraped as int. It returns a dataframe with the film name, letterboxd link,\n",
    "    and letterboxd id\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    html_page = requests.get('https://letterboxd.com/films/ajax/popular/size/small/page/1/')\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "    movies = [{'film_name': movie.find('a', class_='frame')['title'], \n",
    "               'lb_link': 'https://letterboxd.com' + movie.find('a', class_='frame')['href'],\n",
    "               'lb_id': movie.find('div')['data-film-id']}\n",
    "              for movie in soup.find_all('li', class_='listitem poster-container')]\n",
    "\n",
    "    df_film = pd.DataFrame(movies)\n",
    "\n",
    "\n",
    "    # Add pages 2-3000 to df_film\n",
    "    for x in range(2, last_page):\n",
    "        x = str(x)\n",
    "        html_page = requests.get('https://letterboxd.com/films/ajax/popular/size/small/page/' + x)\n",
    "        soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    \n",
    "        movies = [{'film_name': movie.find('a', class_='frame')['title'], \n",
    "                   'lb_link': 'https://letterboxd.com' + movie.find('a', class_='frame')['href'],\n",
    "                   'lb_id': movie.find('div')['data-film-id']}\n",
    "                  for movie in soup.find_all('li', class_='listitem poster-container')]\n",
    "\n",
    "    \n",
    "        df_temp = pd.DataFrame(movies)\n",
    "        df_film = df_film.append(df_temp)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'This took {round((end - start), 2)} seconds to run')\n",
    "    \n",
    "    return df_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the first 3000 most popular films pages. This results with over 70,000 films\n",
    "df_film = film_names(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of film names scraped\n",
    "df_film['lb_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function adds director, whether the 'film' is a TV series or a movie,  and tmbd id to the film data base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dir_tmdb(df):\n",
    "    \"\"\"Takes the film_names df and scrapes each individual film page to return \n",
    "    a data frame with director, tmdb id, and whether it is a movie or tv show.\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    directors =[]\n",
    "    tmdb_ids = []\n",
    "    movie_or_tv = []\n",
    "\n",
    "    for x in range(0, len(df_film)):\n",
    "        html = df_film.iloc[x]['lb_link']\n",
    "        html_page = requests.get(html)\n",
    "        film = BeautifulSoup(html_page.content, 'html.parser')\n",
    "        temp = film.find('a', {'data-track-action': 'TMDb'})['href'].split('/')\n",
    "        movie_tv = temp[-3]\n",
    "        tmdb = temp[-2]\n",
    "        temp2 = film.find('div', class_='tabbed-content-block column-block')\n",
    "        temp2 = temp2.find('a', class_='text-slug').text\n",
    "        directors.append(temp2)\n",
    "        tmdb_ids.append(tmdb)\n",
    "        movie_or_tv.append(movie_tv)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    df_film['director'] = directors\n",
    "    df_film['tmdb_id'] = tmdb_ids\n",
    "    df_film['movie_tv'] = movie_or_tv\n",
    "    \n",
    "    print(f'This took {round((end - start), 2)} seconds to run')\n",
    "\n",
    "    return df_film\n",
    "    # Took 53036.17 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "df_film = add_dir_tmdb(df_film)\n",
    "df_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year to dataframe\n",
    "\n",
    "film_ = list(df_film['film_name'])\n",
    "film_year = [film_[0][-5:-1]]\n",
    "\n",
    "for x in range(1, len(film_)):\n",
    "    film_year.append(film_[x][-5:-1])\n",
    "\n",
    "df_film['release_year'] = film_year    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strip year from title\n",
    "\n",
    "df_film['film_name'] = df_film['film_name'].str[:-7]\n",
    "df_film.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns\n",
    "df_film = df_film[['film_name', 'lb_id', 'lb_link', 'tmdb_id', 'movie_tv', 'Year', 'Director']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_film to computer\n",
    "df_film.to_csv(r'panda_dataframes/letterboxd_film_data_director.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Individual User DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates dataframes of individual users. It is not currently being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_df(username):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Scrape the first page of films watched by username\n",
    "    html_page = requests.get('https://letterboxd.com/'+ username + '/films/page/' + '1' )\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    \n",
    "    # Find the last page number of films watched by usename\n",
    "    pages = soup.find('div', class_=\"paginate-pages\")\n",
    "    last_page = pages.find_all('li', class_='paginate-page')\n",
    "    last_page = last_page[-1].text\n",
    "    \n",
    "    # Create dictionary of film_id, film_name, and link\n",
    "    movies = [{'film_id': movie.find('div', class_='film-poster')['data-film-id'],\n",
    "          'film_name': movie.find('img', class_='image')['alt'], \n",
    "          'link': \"https://letterboxd.com\" + movie.find('div', class_= 'film-poster')['data-target-link']} \n",
    "              for movie in soup.find_all('li', class_='poster-container')]\n",
    "    \n",
    "    # Create Dataframe with film_id, film_name, link\n",
    "    df = pd.DataFrame(movies)\n",
    "    \n",
    "    film_rating = []\n",
    "    user_rating = []\n",
    "\n",
    "    # Create list of star rating\n",
    "    for x in soup.find_all('li', class_='poster-container'):\n",
    "        try: \n",
    "            film_rating.append(x.find('span', class_='rating').text)\n",
    "        except: \n",
    "            film_rating.append(None)\n",
    "\n",
    "    # Change star rating to number rating        \n",
    "    for x in film_rating:\n",
    "        try:\n",
    "            if x[-1]=='½':\n",
    "                user_rating.append(len(x)-0.5)\n",
    "            else:\n",
    "                user_rating.append(len(x))\n",
    "        except:\n",
    "            user_rating.append(None)\n",
    "            \n",
    "    # Add user_rating to the df\n",
    "    df['user_rating'] = user_rating\n",
    "    \n",
    "    # Scrape remaining pages and add to Dataframe\n",
    "    lp = int(last_page)\n",
    "    for y in range(2, lp+1):\n",
    "        z = str(y)\n",
    "\n",
    "        html_page2 = requests.get('https://letterboxd.com/'+ username + '/films/page/' + z )\n",
    "        soup2 = BeautifulSoup(html_page2.content, 'html.parser')\n",
    "        \n",
    "        # Create dictionary of film_id, film_name, and link\n",
    "        movies2 = [{'film_id': movie.find('div', class_='film-poster')['data-film-id'],\n",
    "              'film_name': movie.find('img', class_='image')['alt'], \n",
    "              'link': \"https://letterboxd.com\" + movie.find('div', class_= 'film-poster')['data-target-link']} \n",
    "                  for movie in soup2.find_all('li', class_='poster-container')]\n",
    "        df_temp = pd.DataFrame(movies2)\n",
    "    \n",
    "        film_rating2 = []\n",
    "        user_rating2 = []\n",
    "        \n",
    "        # Create list of star rating\n",
    "        for x in soup2.find_all('li', class_='poster-container'):\n",
    "            try: \n",
    "                film_rating2.append(x.find('span', class_='rating').text)\n",
    "            except: \n",
    "                film_rating2.append(None)\n",
    "\n",
    "        # Change star rating to number rating        \n",
    "        for x in film_rating2:\n",
    "            try:\n",
    "                if x[-1]=='½':\n",
    "                    user_rating2.append(len(x)-0.5)\n",
    "                else:\n",
    "                    user_rating2.append(len(x))\n",
    "            except:\n",
    "                user_rating2.append(None)\n",
    "            \n",
    "        # add user_rating to the df\n",
    "        df_temp['user_rating'] = user_rating2\n",
    "        \n",
    "        \n",
    "        df = df.append(df_temp, ignore_index=True)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'This took {round((end - start), 2)} seconds to run')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for username = creepergnome\n",
    "creepergnome = make_user_df('creepergnome')\n",
    "creepergnome.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SQL Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will take the data frames made above and add them to a created SQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataframes\n",
    "df_rating = pd.read_csv('panda_dataframes/user_rating_3col.csv')\n",
    "df_film = pd.read_csv('panda_dataframes/letterboxd_film_data_director.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the columns of both data frames\n",
    "df_film.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inport and create an engine \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///letterboxd.db', echo=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes the dataframe to make a table in our database\n",
    "def create_sql_table(df, table_name, engine):\n",
    "    df.to_sql(table_name, con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tables to database\n",
    "create_sql_table(df_rating, 'ratings', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add table to database\n",
    "create_sql_table(df_film, 'films', engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
